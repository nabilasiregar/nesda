{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.data_analysis import DataAnalysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('../data/data_wave1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_extract = [\n",
    "    'aApoB', 'aHDL_C', 'aTotFA', 'aSerum_TG', 'aGp', 'aIle', 'Sexe', 'Age',\n",
    "    'aauditsc', 'aedu', 'AIPMETO2', 'asmokstat', 'acidep09', 'anumdis_treat', 'abaiscal', 'ahsCRP', 'aIL6',\n",
    "    'aIRSsum9', 'ams_waist', 'ams_hpt', 'ams_trig2', 'ams_hdl2', 'ams_gluc2',\n",
    "    'amet_syn2', 'atri_med', 'ahdl_med', 'asbp_med', 'adbp_med', 'agluc_med'\n",
    "]\n",
    "\n",
    "extracted_df = original_df[columns_to_extract]\n",
    "\n",
    "extracted_csv_file = '../data/wave1_data_to_discretize.csv'\n",
    "extracted_df.to_csv(extracted_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(df):\n",
    "    \"\"\"\n",
    "    Classify each column in a pandas DataFrame as continuous, discrete, binary, or categorical.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to classify.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with column names as keys and their classifications as values.\n",
    "    \"\"\"\n",
    "    def classify_series(series):\n",
    "        \"\"\"\n",
    "        Classify a pandas Series as continuous, discrete, binary, or categorical.\n",
    "        \n",
    "        Parameters:\n",
    "        series (pd.Series): The Series from a column to classify.\n",
    "\n",
    "        Returns:\n",
    "        str: The classification of the series ('continuous', 'discrete', 'binary', 'categorical').\n",
    "        \"\"\"\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            unique_count = series.nunique()\n",
    "            total_count = len(series)\n",
    "            \n",
    "            if unique_count == 2:\n",
    "                return 'binary'\n",
    "            elif unique_count < 10:\n",
    "                return 'discrete'\n",
    "            else:\n",
    "                return 'continuous'\n",
    "        else:\n",
    "            return 'categorical'\n",
    "\n",
    "    column_classifications = {col: classify_series(df[col]) for col in df.columns}\n",
    "    return column_classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/wave1_data_to_discretize.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace -1 in columns with NaN\n",
    "columns_with_minus_one = [\n",
    "    'ahsCRP', 'aIL6', 'ams_waist', 'ams_hpt', 'ams_trig2', 'ams_hdl2', 'ams_gluc2',\n",
    "    'amet_syn2'\n",
    "]\n",
    "\n",
    "for col in columns_with_minus_one:  \n",
    "  data[col] = data[col].replace(-1, np.nan)\n",
    "\n",
    "# in sleeping pattern, -3 and -2 means no data\n",
    "data['aIRSsum9'] = data[col].replace(-3, np.nan)\n",
    "data['aIRSsum9'] = data[col].replace(-2, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = classify_data(data)\n",
    "column_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalysis.plot_missing_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def perform_little_mcar_test(df):\n",
    "    results = []\n",
    "\n",
    "    for col_with_missing_data in df.columns[df.isnull().any()]:\n",
    "        # Create a mask indicating where data is missing\n",
    "        missing_data = df[col_with_missing_data ].isnull().astype(int)\n",
    "\n",
    "        # Create contingency table for Chi-Squared test\n",
    "        contingency_table = pd.crosstab(missing_data, df.drop(columns=[col_with_missing_data]).isnull().any(axis=1).astype(int))\n",
    "\n",
    "        # Perform Chi-Squared test\n",
    "        chi2_stat, p_val, dof, _ = chi2_contingency(contingency_table, correction=False)\n",
    "\n",
    "        # Append results\n",
    "        results.append({'Column': col_with_missing_data, 'Chi-Squared': chi2_stat, 'df': dof, 'p-value': p_val})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "perform_little_mcar_test(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is not missing completely at random so we cannot discard it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalysis.plot_correlation_matrix(data, columns_to_extract, 'Variables to Construct Bayesian Network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to determine the imputation strategy for each column with missing data based on correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr().abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "def impute_missing_values(df, corr_matrix, column_types):\n",
    "    imputed_df = df.copy()\n",
    "\n",
    "    if 'amet_syn2' in df.columns:\n",
    "        correlated_vars = ['ams_trig2', 'ams_hdl2', 'ams_hpt', 'ams_gluc2', 'atri_med', 'ahdl_med', 'asbp_med', 'adbp_med', 'agluc_med']\n",
    "        imputation_vars = ['amet_syn2'] + correlated_vars\n",
    "        data_for_imputation = df[imputation_vars]\n",
    "        \n",
    "        iter_imputer = IterativeImputer(max_iter=100, random_state=42)\n",
    "        imputed_data = iter_imputer.fit_transform(data_for_imputation)\n",
    "        # convert to binary\n",
    "        imputed_df['amet_syn2'] = np.round(imputed_data[:, 0]).astype(int)\n",
    "\n",
    "    for col in df.columns[df.isnull().any()]:\n",
    "        if col == 'amet_syn2':\n",
    "            continue\n",
    "\n",
    "        col_corr = corr_matrix[col].drop(col)\n",
    "        max_corr = col_corr.max()\n",
    "\n",
    "        # use MICE for highly correlated columns\n",
    "        if max_corr > 0.6:\n",
    "            mice_imputer = IterativeImputer(random_state=42)\n",
    "            imputed_df[col] = mice_imputer.fit_transform(df[[col] + col_corr.index.tolist()])[:, 0]\n",
    "        # use kNN for everything else\n",
    "        else:\n",
    "            knn_imputer = KNNImputer(n_neighbors=10)\n",
    "            imputed_df[col] = knn_imputer.fit_transform(df[[col] + col_corr.index.tolist()])[:, 0]\n",
    "        \n",
    "        if column_types[col] == 'binary':\n",
    "            imputed_df[col] = np.round(imputed_df[col]).astype(int)\n",
    "\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df = impute_missing_values(data, corr_matrix, column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalysis.plot_missing_values(imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_data(imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.to_csv('../data/network/imputed_data_wave1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform continuous variables to discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = pd.read_csv('../data/network/imputed_data_wave1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def apply_kmeans(df, column_name, n_clusters=3, random_state=0):\n",
    "    data = df[column_name].values.reshape(-1, 1)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(data)\n",
    "    \n",
    "    # get centroids and create sorted labels\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    sorted_idx = np.argsort(centroids.ravel())\n",
    "    labels_sorted = ['Low', 'Moderate', 'High']\n",
    "    \n",
    "    # map original cluster labels to meaningful labels\n",
    "    cluster_mapping = {old_label: new_label for old_label, new_label in zip(sorted_idx, labels_sorted)}\n",
    "    df[column_name] = kmeans.labels_\n",
    "    df[column_name] = df[column_name].map(cluster_mapping)\n",
    "    \n",
    "    # visualize\n",
    "    colors = ['#3B28CC', '#3F8EFC', '#ADD7F6']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for label in labels_sorted:\n",
    "        subset = df[df[column_name] == label]\n",
    "        plt.hist(subset[column_name], bins=np.arange(n_clusters + 1) - 0.5, color=colors[labels_sorted.index(label)], label=label, alpha=0.75, edgecolor='black')\n",
    "\n",
    "    plt.title(f'Clusters of {column_name}')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(range(n_clusters), labels_sorted) \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aApoB\n",
    "bins = [-float('inf'), 1.3, float('inf')]\n",
    "labels = ['Low', 'High']\n",
    "preprocessed_df['aApoB'] = pd.cut(preprocessed_df['aApoB'], bins=bins, labels=labels, right=False)\n",
    "preprocessed_df['aApoB'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aTotFA\n",
    "preprocessed_df = apply_kmeans(preprocessed_df, 'aTotFA', n_clusters=3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df['aHDL_C'] = preprocessed_df.apply(\n",
    "    lambda row: 'Desirable' if (row['Sexe'] == 1 and row['aHDL_C'] > 1.03) or (row['Sexe'] == 2 and row['aHDL_C'] > 1.29) else 'At risk',\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-float('inf'), 1.69, 2.26, 5.65, float('inf')]\n",
    "labels = ['Desirable', 'Mild Hypertriglyceridemia', 'High Hypertriglyceridemia', 'Very High Hypertriglyceridemia']\n",
    "preprocessed_df['aSerum_TG'] = pd.cut(preprocessed_df['aSerum_TG'], bins=bins, labels=labels)\n",
    "preprocessed_df['aSerum_TG'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df['aGp'] = preprocessed_df['aGp'].apply(lambda x: 'Normal' if x <= 1.2 else 'At risk')\n",
    "preprocessed_df['aGp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = apply_kmeans(preprocessed_df, 'aIle', n_clusters=3, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_met(met):\n",
    "    moderate = 4.9 * 60 * 3 # assuming 3 sessions of 1 hour each per week\n",
    "    vigorous = 6.8 * 60 * 3\n",
    "    if met >= moderate and met < vigorous:  \n",
    "        return 'Moderate'\n",
    "    elif met >= 6.8 * 60 * 3: \n",
    "        return 'Vigorous'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "preprocessed_df['AIPMETO2'] = preprocessed_df['AIPMETO2'].apply(categorize_met)\n",
    "print(preprocessed_df['AIPMETO2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df['aIL6'] = pd.cut(preprocessed_df['aIL6'],\n",
    "                                bins=[-float('inf'), 7, float('inf')],\n",
    "                                labels=['Normal', 'High'],\n",
    "                                right=True) \n",
    "\n",
    "print(preprocessed_df['aIL6'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df['ahsCRP'] = pd.cut(preprocessed_df['ahsCRP'],\n",
    "                                  bins=[-float('inf'), 0.3, 1, 10, 50, float('inf')],\n",
    "                                  labels=['Normal', 'Minor', 'Moderate', 'Marked', 'Severe'],\n",
    "                                  right=False)\n",
    "print(preprocessed_df['ahsCRP'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_atri_med(row):\n",
    "    adult_threshold = 150 * 0.01129  # mg/dL to mmol/L for adults\n",
    "    child_threshold = 90 * 0.01129   # mg/dL to mmol/L for children and teens\n",
    "\n",
    "    if row['Age'] > 19:\n",
    "        if row['atri_med'] < adult_threshold:\n",
    "            return 'Healthy'\n",
    "        else:\n",
    "            return 'High'\n",
    "    else: \n",
    "        if row['atri_med'] < child_threshold:\n",
    "            return 'Healthy'\n",
    "        else:\n",
    "            return 'High'\n",
    "\n",
    "preprocessed_df['atri_med'] = preprocessed_df.apply(categorize_atri_med, axis=1)\n",
    "print(preprocessed_df['atri_med'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_ahdl_med(row):\n",
    "    if row['Sexe'] == 1: \n",
    "        return 'Healthy' if row['ahdl_med'] > 1.0 else 'Unhealthy'\n",
    "    elif row['Sexe'] == 2: \n",
    "        return 'Healthy' if row['ahdl_med'] > 1.2 else 'Unhealthy'\n",
    "\n",
    "preprocessed_df['ahdl_med'] = preprocessed_df.apply(categorize_ahdl_med, axis=1)\n",
    "preprocessed_df['ahdl_med'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_blood_pressure(df):\n",
    "    for index, row in df.iterrows():\n",
    "        sbp = row['asbp_med']\n",
    "        dbp = row['adbp_med']\n",
    "        \n",
    "        if sbp < 122 and dbp < 80:\n",
    "            df.at[index, 'asbp_med'] = 'Normal'\n",
    "            df.at[index, 'adbp_med'] = 'Normal'\n",
    "        elif 120 <= sbp <= 129 and dbp < 80:\n",
    "            df.at[index, 'asbp_med'] = 'Elevated'\n",
    "            df.at[index, 'adbp_med'] = 'Elevated'\n",
    "        elif (130 <= sbp <= 139 or 80 <= dbp <= 89):\n",
    "            df.at[index, 'asbp_med'] = 'Hypertension I'\n",
    "            df.at[index, 'adbp_med'] = 'Hypertension I'\n",
    "        elif sbp >= 140 or dbp >= 90:\n",
    "            df.at[index, 'asbp_med'] = 'Hypertension II'\n",
    "            df.at[index, 'adbp_med'] = 'Hypertension II'\n",
    "        else:\n",
    "            df.at[index, 'asbp_med'] = 'Uncategorized'\n",
    "            df.at[index, 'adbp_med'] = 'Uncategorized'\n",
    "\n",
    "categorize_blood_pressure(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_glucose_level(glucose_mmol):\n",
    "    glucose_mg_dl = glucose_mmol * 18  # convert mmol/L to mg/dL\n",
    "    if glucose_mg_dl < 70:\n",
    "        return 'Hypoglycemia'\n",
    "    elif 70 <= glucose_mg_dl <= 100:\n",
    "        return 'Normal'\n",
    "    elif 100 < glucose_mg_dl <= 125:\n",
    "        return 'Prediabetes'\n",
    "    elif glucose_mg_dl >= 126:\n",
    "        return 'Diabetes'\n",
    "    else:\n",
    "        return 'Uncategorized' \n",
    "\n",
    "preprocessed_df['agluc_med'] = preprocessed_df['agluc_med'].apply(categorize_glucose_level)\n",
    "\n",
    "preprocessed_df['agluc_med'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to build the network\n",
    "preprocessed_df.to_csv('../data/network/final_data_wave1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.read_csv('../data/network/final_data_wave1.csv')\n",
    "classify_data(final_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
